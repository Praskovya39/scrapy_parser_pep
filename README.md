# Парсинг документов PEP

Асинхронный парсер документов PEP на базе фреймворка Scrapy, собирающий данные о
PEP с сайта `https://www.python.org/`.
С каждой страницы PEP парсер собирает номер, название, статус и сохраняет
два файла в формате `.csv` в папке `results/...`.

* В первый файл сохраняет список всех PEP: номер, название и статус.
  Во второй файл подсчитывает общее количество каждого статуса и сумму всех
  статусов.
  В последней строке файла в колонке «Статус» общее количество всех документов.
* Метод паука parse() собирает ссылки на документы PEP.
  Метод parse_pep() парсит страницы с документами и формирует Items.
  При парсинге применены CSS- и XPath-селекторы.
  Для создания Items описан класс PepParseItem.
* Файлы со списком PEP именованы по маске pep_ДатаВремя.csv.
  Файлы со сводкой по статусам именованы по маске status_summary_ДатаВремя.csv.

## Технологии проекта

* Python — высокоуровневый язык программирования.
* Scrapy — популярный фреймворк для парсинга веб сайтов. Особенности:
    * Многопоточность
    * Веб-краулер для перехода от ссылки к ссылке
    * Извлечение данных
    * Проверка данных
    * Сохранение в другой формат/базу данных
* XPath — язык запросов к элементам XML-документа. 
* CSS - Cascading Style Sheets, каскадные таблицы стилей.
* Pytest — среда тестирования, основанная на Python. 

## Инструкция по развёртыванию проекта

Клонировать репозиторий и перейти в него в командной строке:


Создать и активировать виртуальное окружение:

```
python -m venv env

source venv/bin/activate
```

Установить зависимости из файла requirements.txt:

```
python -m pip install --upgrade pip

pip install -r requirements.txt
```

## Запуск парсера

```
scrapy crawl pep
```

### Зависимости:

* Python 3.9
* Scrapy